---
title: "The problem with citation count as an impact metric"
date: 2019-10-18
categories: 
 - "blog"
tags: 
 - "barabasi"
 - "impact"
cover_image: "/assets/img/2017/10/patricia-serna-415257.jpg"
layout: "post"
---

Inspired by [A citation is not a citation is not a citation](https://liorpachter.wordpress.com/2017/01/18/a-citation-is-not-a-citation-is-not-a-citation/) by Lior Patcher, this rant is about metrics.

Lior Patcher is a researcher in Caltech. As many other researchers in the academy, Dr. Patcher is measured by, among other things, publications and their impact as measured by citations. In his post, Lior Patcher criticised both the current impact metrics and also their effect on citation patterns in the academic community. 

PROBLEM POINTED: citations don't really measure "actual" citations. Most of the appeared citations are "hit and run citations" i.e: people mention other people's research without taking anything from that research. 

> 
> 
> In fact this author has cited [a certain] work in exactly the same way in several other papers which appear to be copies of each other for a total of 7 citations all of which are placed in dubious “papers”. I suppose one may call this sort of thing **hit and run citation**.


via [A citation is not a citation is not a citation — Bits of DNA](https://liorpachter.wordpress.com/2017/01/18/a-citation-is-not-a-citation-is-not-a-citation/)

I think that the biggest problem with citation counts is that it costs nothing to cite a paper. When you add a research (or a post, for that matter) to your reference list, you know that most probably nobody will check whether actually read it, that nobody will check whether you got that publication correctly and that nobody will   that the chances are super (SUUPER) low  nobody will check whether you conclusions are right. All it takes is to click a button.
