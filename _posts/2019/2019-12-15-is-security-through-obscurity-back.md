---
title: "Is security through obscurity back?"
date: 2019-12-15
categories: 
 - "blog"
tags: 
 - "blackbox"
 - "hbr"
 - "machine-learning"
 - "opinion"
 - "transparency"
cover_image: "/assets/img/2019/12/featured_blackbox.png"
layout: "post"
---

<!-- wp:paragraph -->
HBR published an opinion post by Andrew Burt, called "[The AI Transparency Paradox](https://hbr.org/2019/12/the-ai-transparency-paradox)." This post talks about the problems that were created by tools that open up the "black box" of a machine learning model.


<!-- /wp:paragraph -->

<!-- wp:paragraph -->
"Black box" refers to the situation where one can't explain why a machine learning model predicted whatever it predicted. Predictability is not only important when one wants to improve the model or to pinpoint mistakes, but it is also an essential feature in many fields. For example, when I was developing a cancer detection model, every physician requested to know why we thought a particular patient had cancer. That is why I'm happy, so many people develop tools that allow peeking into the black box.


<!-- /wp:paragraph -->

<!-- wp:paragraph -->
I was very surprised to read the "transparency paradox" post. Not because I couldn't imagine that people will use the insights to hack the models. I was surprised because the post reads like a case for [security through obscurity](https://en.wikipedia.org/wiki/Security_through_obscurity) -- an ancient practice that was mostly eradicated from the mainstream. 


<!-- /wp:paragraph -->

<!-- wp:paragraph -->
Yes, ML transparency opens opportunities for hacking and abuse. However, this is EXACTLY the reason why such openness is needed. Hacking attempts will not disappear with transparency removal; they will be harder to defend. 


<!-- /wp:paragraph -->
