---
title: "On machine learning, job security, professional pride, and network trolling"
date: 2017-11-21
categories: 
 - "blog"
tags: 
 - "data-science"
 - "deep-learning"
 - "job-security"
 - "machine-learning"
 - "neural-networks"
layout: "post"
---

If you are a data scientist, I am sure you wondered whether deep neural networks will replace you at your job one day. Every time I read about reports of researchers who managed to trick neural networks, I wonder whether the researchers were thinking about their job security, or their professional pride while performing the experiments. I think that the first example of such a report is a 2014 paper by Christian Szegedy and his colleagues called "[Intriguing properties of neural networks](https://arxiv.org/abs/1312.6199)". The main goal of this paper, so it seems, was to peek into the black box of neural networks. In one of the experiments, the authors designed minor, invisible perturbation of the original images. These perturbations diminished the classification accuracy of a trained model.

![Screen Shot 2017-11-21 at 16.50.05.png](/assets/img/2017/11/screen-shot-2017-11-21-at-16-50-05.png){:width="1184" :class="alignnone"}

In the recent post "[5 Ways to Troll Your Neural Network](https://mathwithbaddrawings.com/2017/10/18/5-ways-to-troll-your-neural-network/)" Ben Orlin describes five different ways to "troll a network".

<small>Image credit: Figure 5 from "<a href="https://arxiv.org/abs/1312.6199">Intriguing properties of neural networks</a>".</small>
